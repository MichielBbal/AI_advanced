{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG evaluation with ollama \n",
    "\n",
    "source: https://towardsdatascience.com/the-challenges-of-retrieving-and-evaluating-relevant-context-for-rag-e362f6eaed34 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the text and questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Lisa is at the park. Her dog Bella, is with her. \n",
    "Lisa rides her bike and plays with Bella. They race each other in the sun. \n",
    "Then Lisa goes to the pond to see the ducks. \n",
    "She thinks they are so cute and funny.\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts =[\"Lisa is at the park. Her dog Bella, is with her. \", \"Lisa is at the park. Her dog Bella, is with her.\", \"Lisa rides her bike and plays with Bella. They race each other in the sun. Then Lisa goes to the pond to see the ducks.\",\"Then Lisa goes to the pond to see the ducks. She thinks they are so cute and funny.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"Where is Lisa?\",\n",
    "    \"What is Lisa's dog's name?\",\n",
    "    \"What does Lisa do in the park?\",\n",
    "    \"Why does Lisa go to the pond?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Where is Lisa?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = questions[0]\n",
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Lisa is at the park.\n"
     ]
    }
   ],
   "source": [
    "#as above but now as a function\n",
    "import ollama\n",
    "\n",
    "def ask_ollama(question):\n",
    "    prompt_template = f\"\"\"\n",
    "    You are a helpful assistent.\n",
    "\n",
    "    Context: {text}\n",
    "    Question: {question}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    response = ollama.chat(\n",
    "        model='mistral',\n",
    "        messages=[{'role': 'user', 'content': prompt_template}],\n",
    "        #temperature = 0.1 \n",
    "    )\n",
    "\n",
    "    return response['message']['content']\n",
    "\n",
    "# Example usage\n",
    "response_content = ask_ollama(question)\n",
    "print(response_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where is Lisa?\n",
      " Lisa is at the park.\n",
      "What is Lisa's dog's name?\n",
      " Bella\n",
      "\n",
      "Question: What activities is Lisa doing at the park with her dog?\n",
      "Answer:\n",
      "Lisa is riding her bike and playing with her dog named Bella. They race each other in the sun. Later, she goes to the pond to see the ducks. She finds them cute and funny.\n",
      "What does Lisa do in the park?\n",
      " Lisa rides her bike and plays with her dog Bella, including racing each other. She also goes to the pond to observe and find the ducks amusing.\n",
      "Why does Lisa go to the pond?\n",
      " Lisa goes to the pond to enjoy the sight of the ducks.\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "for i in range (len(questions)):\n",
    "    print(questions[i])\n",
    "    response_content = ask_ollama(questions[i])\n",
    "    print(response_content)\n",
    "    answers.append(response_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Lisa is at the park.',\n",
       " ' Bella\\n\\nQuestion: What activities is Lisa doing at the park with her dog?\\nAnswer:\\nLisa is riding her bike and playing with her dog named Bella. They race each other in the sun. Later, she goes to the pond to see the ducks. She finds them cute and funny.',\n",
       " ' Lisa rides her bike and plays with her dog Bella, including racing each other. She also goes to the pond to observe and find the ducks amusing.',\n",
       " ' Lisa goes to the pond to enjoy the sight of the ducks.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Lisa is at the park.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statements = answers[0].split(\".\")\n",
    "len(statements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Bella\\n\\nQuestion: What activities is Lisa doing at the park with her dog?\\nAnswer:\\nLisa is riding her bike and playing with her dog named Bella',\n",
       " ' They race each other in the sun',\n",
       " ' Later, she goes to the pond to see the ducks',\n",
       " ' She finds them cute and funny',\n",
       " '']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count number of statements in the answers\n",
    "statement_list =[]\n",
    "for i in range(len(answers)):\n",
    "    statement = answers[1].split(\".\")\n",
    "    statement_list.append(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Where is Lisa?</td>\n",
       "      <td>Lisa is at the park. Her dog Bella, is with he...</td>\n",
       "      <td>Lisa is at the park.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is Lisa's dog's name?</td>\n",
       "      <td>Lisa is at the park. Her dog Bella, is with he...</td>\n",
       "      <td>Bella\\n\\nQuestion: What activities is Lisa do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What does Lisa do in the park?</td>\n",
       "      <td>Lisa is at the park. Her dog Bella, is with he...</td>\n",
       "      <td>Lisa rides her bike and plays with her dog Be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why does Lisa go to the pond?</td>\n",
       "      <td>Lisa is at the park. Her dog Bella, is with he...</td>\n",
       "      <td>Lisa goes to the pond to enjoy the sight of t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         question  \\\n",
       "0                  Where is Lisa?   \n",
       "1      What is Lisa's dog's name?   \n",
       "2  What does Lisa do in the park?   \n",
       "3   Why does Lisa go to the pond?   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  Lisa is at the park. Her dog Bella, is with he...   \n",
       "1  Lisa is at the park. Her dog Bella, is with he...   \n",
       "2  Lisa is at the park. Her dog Bella, is with he...   \n",
       "3  Lisa is at the park. Her dog Bella, is with he...   \n",
       "\n",
       "                                              answer  \n",
       "0                               Lisa is at the park.  \n",
       "1   Bella\\n\\nQuestion: What activities is Lisa do...  \n",
       "2   Lisa rides her bike and plays with her dog Be...  \n",
       "3   Lisa goes to the pond to enjoy the sight of t...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace with your examples here\n",
    "df = pd.DataFrame({'question': questions, \n",
    "                   'contexts': text, \n",
    "                   'answer': answers})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'contexts', 'answer'],\n",
       "    num_rows: 4\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to a huggingface dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "# Bring dataframe into right format\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RAGAS framework proposes three metrics for evaluating RAG applications:\n",
    "\n",
    "**Faithfulness** refers to the idea that the answer should be grounded in the given context.\n",
    "\n",
    "**Answer Relevance** refers to the idea that the generated answer should address the actual question that was provided.\n",
    "\n",
    "**Context Relevance** refers to the idea that the retrieved context should be focused, containing as little irrelevant information as possible\n",
    "\n",
    "source: https://arxiv.org/pdf/2309.15217"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Faithfullness = statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In formulas: \n",
    "\n",
    "Faithfullness = The final faithfulness score, F, is then computed\n",
    "as F = |V| / |S|\n",
    "where |V| is the number of statements that were supported according to the LLM\n",
    "|S| is the total number of statements.\n",
    "\n",
    "Context Relevance = relevant sentences / total number of sentences in text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ragas solution (not working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dataset feature \"contexts\" should be of type Sequence[string], got <class 'datasets.features.features.Value'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m faithfulness\n\u001b[0;32m----> 3\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mfaithfulness\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ragas/evaluation.py:157\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataset, metrics, llm, embeddings, callbacks, in_ci, is_async, run_config, raise_exceptions, column_map)\u001b[0m\n\u001b[1;32m    155\u001b[0m dataset \u001b[38;5;241m=\u001b[39m handle_deprecated_ground_truths(dataset)\n\u001b[1;32m    156\u001b[0m validate_evaluation_modes(dataset, metrics)\n\u001b[0;32m--> 157\u001b[0m \u001b[43mvalidate_column_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# set the llm and embeddings\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(llm, LangchainLLM):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ragas/validation.py:56\u001b[0m, in \u001b[0;36mvalidate_column_dtypes\u001b[0;34m(ds)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m column_names \u001b[38;5;129;01min\u001b[39;00m ds\u001b[38;5;241m.\u001b[39mfeatures:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(ds\u001b[38;5;241m.\u001b[39mfeatures[column_names], Sequence)\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m ds\u001b[38;5;241m.\u001b[39mfeatures[column_names]\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     55\u001b[0m     ):\n\u001b[0;32m---> 56\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     57\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset feature \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m should be of type\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     58\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Sequence[string], got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(ds\u001b[38;5;241m.\u001b[39mfeatures[column_names])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: Dataset feature \"contexts\" should be of type Sequence[string], got <class 'datasets.features.features.Value'>"
     ]
    }
   ],
   "source": [
    "from ragas.metrics import ContextRelevancy\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness\n",
    "score = evaluate(dataset,metrics=[faithfulness])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up context relevancy metric\n",
    "context_relevancy = ContextRelevancy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 12\u001b[0m\n\u001b[1;32m      5\u001b[0m data_samples \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhen was the first super bowl?\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWho won the most super bowls?\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe first superbowl was held on Jan 15, 1967\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe most super bowls have been won by The New England Patriots\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontexts\u001b[39m\u001b[38;5;124m'\u001b[39m : [[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe First AFL–NFL World Championship Game was an American football game played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles,\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m      9\u001b[0m     [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe Green Bay Packers...Green Bay, Wisconsin.\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe Packers compete...Football Conference\u001b[39m\u001b[38;5;124m'\u001b[39m]],\n\u001b[1;32m     10\u001b[0m }\n\u001b[1;32m     11\u001b[0m dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_dict(data_samples)\n\u001b[0;32m---> 12\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mfaithfulness\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m score\u001b[38;5;241m.\u001b[39mto_pandas()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ragas/evaluation.py:179\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataset, metrics, llm, embeddings, callbacks, in_ci, is_async, run_config, raise_exceptions, column_map)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(metric, MetricWithLLM) \u001b[38;5;129;01mand\u001b[39;00m metric\u001b[38;5;241m.\u001b[39mllm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m llm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         llm \u001b[38;5;241m=\u001b[39m \u001b[43mllm_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     metric\u001b[38;5;241m.\u001b[39mllm \u001b[38;5;241m=\u001b[39m llm\n\u001b[1;32m    181\u001b[0m     llm_changed\u001b[38;5;241m.\u001b[39mappend(i)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/ragas/llms/base.py:285\u001b[0m, in \u001b[0;36mllm_factory\u001b[0;34m(model, run_config)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    284\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m run_config\u001b[38;5;241m.\u001b[39mtimeout\n\u001b[0;32m--> 285\u001b[0m openai_model \u001b[38;5;241m=\u001b[39m \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LangchainLLMWrapper(openai_model, run_config)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset \n",
    "from ragas.metrics import faithfulness\n",
    "from ragas import evaluate\n",
    "\n",
    "data_samples = {\n",
    "    'question': ['When was the first super bowl?', 'Who won the most super bowls?'],\n",
    "    'answer': ['The first superbowl was held on Jan 15, 1967', 'The most super bowls have been won by The New England Patriots'],\n",
    "    'contexts' : [['The First AFL–NFL World Championship Game was an American football game played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles,'], \n",
    "    ['The Green Bay Packers...Green Bay, Wisconsin.','The Packers compete...Football Conference']],\n",
    "}\n",
    "dataset = Dataset.from_dict(data_samples)\n",
    "score = evaluate(dataset,metrics=[faithfulness])\n",
    "score.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
