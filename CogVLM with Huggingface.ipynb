{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd85935c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dccc109",
   "metadata": {},
   "source": [
    "source: https://huggingface.co/THUDM/cogvlm-chat-hf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7797188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.1.0\n",
      "  Downloading torch-2.1.0-cp310-none-macosx_10_9_x86_64.whl.metadata (24 kB)\n",
      "Collecting transformers==4.35.0\n",
      "  Downloading transformers-4.35.0-py3-none-any.whl.metadata (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.1/123.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting accelerate==0.24.1\n",
      "  Downloading accelerate-0.24.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: sentencepiece==0.1.99 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (0.1.99)\n",
      "Collecting einops==0.7.0\n",
      "  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting xformers==0.0.22.post7\n",
      "  Downloading xformers-0.0.22.post7.tar.gz (3.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement triton==2.1.0 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for triton==2.1.0\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25hNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch==2.1.0 transformers==4.35.0 accelerate==0.24.1 sentencepiece==0.1.99 einops==0.7.0 xformers==0.0.22.post7 triton==2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "815c68ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xformers==0.0.22\n",
      "  Downloading xformers-0.0.22.tar.gz (3.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from xformers==0.0.22) (1.24.0)\n",
      "Collecting torch==2.0.1 (from xformers==0.0.22)\n",
      "  Downloading torch-2.0.1-cp310-none-macosx_10_9_x86_64.whl (143.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/143.4 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from torch==2.0.1->xformers==0.0.22) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from torch==2.0.1->xformers==0.0.22) (4.4.0)\n",
      "Requirement already satisfied: sympy in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from torch==2.0.1->xformers==0.0.22) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from torch==2.0.1->xformers==0.0.22) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from torch==2.0.1->xformers==0.0.22) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from jinja2->torch==2.0.1->xformers==0.0.22) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages/mpmath-1.2.1-py3.10.egg (from sympy->torch==2.0.1->xformers==0.0.22) (1.2.1)\n",
      "Building wheels for collected packages: xformers\n",
      "  Building wheel for xformers (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[290 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-310\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-310/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/_deprecation_warning.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/version.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/checkpoint.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/test.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/utils.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/_cpp_lib.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/info.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/fused_linear_layer.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/sum_strided.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/vararg_kernel.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/k_activations.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/k_layer_norm.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/k_sum.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/utils.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/k_fused_matmul_fw.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/dropout.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/k_dropout.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/softmax.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/layer_norm.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/k_fused_matmul_bw.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/k_softmax.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-310/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/simplicial_embedding.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/residual.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/reversible.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/activations.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/multi_head_dispatch.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/input_projection.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/patch_embedding.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_mem_eff_attention.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_indexing.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_mlp.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_triton_stride_sum.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_blocksparse_transformers.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_mem_eff_attn_decoder.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_transformer.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_revnet.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_swiglu.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_triton_layernorm.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_causal_blocksparse.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_triton_fused_linear.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_triton_blocksparse.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_triton_softmax.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/utils.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_triton_dropout.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_nystrom_utils.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_attn_decoding.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_multi_head_dispatch.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_sddmm.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_core.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-310/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/rmsnorm.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/swiglu_op.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/unbind.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/rope_padded.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/common.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/indexing.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/ops\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-310/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/device_limits.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/api.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/slow_ops_profiler.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/profiler.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-310/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m copying xformers/sparse/_csr_ops.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m copying xformers/sparse/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m copying xformers/sparse/utils.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m copying xformers/sparse/blocksparse_tensor.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m copying xformers/sparse/csr_tensor.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-310/xformers/helpers\n",
      "  \u001b[31m   \u001b[0m copying xformers/helpers/test_utils.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/helpers\n",
      "  \u001b[31m   \u001b[0m copying xformers/helpers/hierarchical_configs.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/helpers\n",
      "  \u001b[31m   \u001b[0m copying xformers/helpers/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/helpers\n",
      "  \u001b[31m   \u001b[0m copying xformers/helpers/timm_sparse_attention.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/helpers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/fused_softmax.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_blocksparse_attn_interface.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_blocksparse_attention.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/bert_padding.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_triton_og.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_triton.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_interface.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-310/xformers/factory\n",
      "  \u001b[31m   \u001b[0m copying xformers/factory/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/factory\n",
      "  \u001b[31m   \u001b[0m copying xformers/factory/hydra_helper.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/factory\n",
      "  \u001b[31m   \u001b[0m copying xformers/factory/block_factory.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/factory\n",
      "  \u001b[31m   \u001b[0m copying xformers/factory/model_factory.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/factory\n",
      "  \u001b[31m   \u001b[0m copying xformers/factory/block_configs.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/factory\n",
      "  \u001b[31m   \u001b[0m copying xformers/factory/weight_init.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/factory\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/global_tokens.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/ortho.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/blocksparse.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/local.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/compositional.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/pooling.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/_sputnik_sparse.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/core.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/lambda_layer.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/random.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/fourier_mix.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/scaled_dot_product.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/utils.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/attention_mask.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/linformer.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/attention_patterns.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/visual.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/sparsity_config.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/nystrom.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/favor.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/base.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-310/xformers/components/feedforward\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/feedforward/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components/feedforward\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/feedforward/mixture_of_experts.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components/feedforward\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/feedforward/mlp.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components/feedforward\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/feedforward/conv_mlp.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components/feedforward\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/feedforward/fused_mlp.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components/feedforward\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/feedforward/base.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components/feedforward\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-310/xformers/components/positional_embedding\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/positional_embedding/vocab.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components/positional_embedding\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/positional_embedding/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components/positional_embedding\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/positional_embedding/param.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components/positional_embedding\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/positional_embedding/sine.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components/positional_embedding\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/positional_embedding/rotary.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components/positional_embedding\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/positional_embedding/base.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components/positional_embedding\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-310/xformers/components/attention/feature_maps\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/feature_maps/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components/attention/feature_maps\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/feature_maps/softmax.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components/attention/feature_maps\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/feature_maps/base.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/components/attention/feature_maps\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-310/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/batch_submit.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/batch_fetch_results.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/run_with_submitit.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/benchmarks/LRA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/run_tasks.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/benchmarks/LRA\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/run_grid_search.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/benchmarks/LRA\r\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-310/xformers/benchmarks/LRA/code\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/code/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/benchmarks/LRA/code\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/code/model_wrapper.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/benchmarks/LRA/code\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/code/dataset.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/benchmarks/LRA/code\r\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-310/xformers/ops/triton\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/triton/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/ops/triton\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/triton/rope_padded_kernels.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/ops/triton\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/triton/rmsnorm_kernels.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/ops/triton\r\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-310/xformers/ops/fmha\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/decoder.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/ops/fmha\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/triton.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/ops/fmha\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/dispatch.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/ops/fmha\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/ops/fmha\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/attn_bias.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/ops/fmha\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/common.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/ops/fmha\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/flash.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/ops/fmha\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/small_k.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/ops/fmha\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/cutlass.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/ops/fmha\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/triton_splitk.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/ops/fmha\r\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/losses\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/losses/cross_entropy.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/losses\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/losses/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/losses\r\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/layers\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/layers/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/layers\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/layers/patch_embed.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/layers\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/layers/rotary.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/layers\r\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/utils\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/utils/pretrained.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/utils\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/utils/generation.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/utils\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/utils/benchmark.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/utils\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/utils/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/utils\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/utils/distributed.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/utils\r\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/models\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/bigcode.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/models\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/gptj.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/models\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/models\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/opt.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/models\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/llama.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/models\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/vit.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/models\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/baichuan.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/models\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/bert.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/models\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/falcon.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/models\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/gpt_neox.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/models\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/gpt.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/models\r\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/ops\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/activations.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/ops\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/ops\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/fused_dense.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/ops\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/rms_norm.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/ops\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/layer_norm.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/ops\r\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/modules\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/modules/embedding.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/modules\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/modules/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/modules\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/modules/mlp.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/modules\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/modules/block.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/modules\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/modules/mha.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/modules\r\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/ops/triton\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/triton/cross_entropy.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/ops/triton\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/triton/linear.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/ops/triton\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/triton/k_activations.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/ops/triton\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/triton/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/ops/triton\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/triton/mlp.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/ops/triton\r\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/triton/rotary.py -> build/lib.macosx-10.9-x86_64-cpython-310/xformers/_flash_attn/ops/triton\r\n",
      "  \u001b[31m   \u001b[0m running build_ext\r\n",
      "  \u001b[31m   \u001b[0m building 'xformers._C' extension\r\n",
      "  \u001b[31m   \u001b[0m creating /private/var/folders/cj/qtbz9fvd3svc0x28yv2756mh0000gn/T/pip-install-depslgqw/xformers_929eab129bff4cf7b54c3df983ea1a88/build/temp.macosx-10.9-x86_64-cpython-310\r\n",
      "  \u001b[31m   \u001b[0m creating /private/var/folders/cj/qtbz9fvd3svc0x28yv2756mh0000gn/T/pip-install-depslgqw/xformers_929eab129bff4cf7b54c3df983ea1a88/build/temp.macosx-10.9-x86_64-cpython-310/xformers\r\n",
      "  \u001b[31m   \u001b[0m creating /private/var/folders/cj/qtbz9fvd3svc0x28yv2756mh0000gn/T/pip-install-depslgqw/xformers_929eab129bff4cf7b54c3df983ea1a88/build/temp.macosx-10.9-x86_64-cpython-310/xformers/csrc\r\n",
      "  \u001b[31m   \u001b[0m creating /private/var/folders/cj/qtbz9fvd3svc0x28yv2756mh0000gn/T/pip-install-depslgqw/xformers_929eab129bff4cf7b54c3df983ea1a88/build/temp.macosx-10.9-x86_64-cpython-310/xformers/csrc/attention\r\n",
      "  \u001b[31m   \u001b[0m creating /private/var/folders/cj/qtbz9fvd3svc0x28yv2756mh0000gn/T/pip-install-depslgqw/xformers_929eab129bff4cf7b54c3df983ea1a88/build/temp.macosx-10.9-x86_64-cpython-310/xformers/csrc/attention/autograd\r\n",
      "  \u001b[31m   \u001b[0m creating /private/var/folders/cj/qtbz9fvd3svc0x28yv2756mh0000gn/T/pip-install-depslgqw/xformers_929eab129bff4cf7b54c3df983ea1a88/build/temp.macosx-10.9-x86_64-cpython-310/xformers/csrc/attention/cpu\r\n",
      "  \u001b[31m   \u001b[0m creating /private/var/folders/cj/qtbz9fvd3svc0x28yv2756mh0000gn/T/pip-install-depslgqw/xformers_929eab129bff4cf7b54c3df983ea1a88/build/temp.macosx-10.9-x86_64-cpython-310/xformers/csrc/indexing\r\n",
      "  \u001b[31m   \u001b[0m creating /private/var/folders/cj/qtbz9fvd3svc0x28yv2756mh0000gn/T/pip-install-depslgqw/xformers_929eab129bff4cf7b54c3df983ea1a88/build/temp.macosx-10.9-x86_64-cpython-310/xformers/csrc/swiglu\r\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\r\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\r\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\r\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/cj/qtbz9fvd3svc0x28yv2756mh0000gn/T/pip-install-depslgqw/xformers_929eab129bff4cf7b54c3df983ea1a88/setup.py\", line 419, in <module>\r\n",
      "  \u001b[31m   \u001b[0m     setuptools.setup(\r\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/michielbontenbal/anaconda3/lib/python3.10/site-packages/setuptools/__init__.py\", line 87, in setup\r\n",
      "  \u001b[31m   \u001b[0m     return distutils.core.setup(**attrs)\r\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/michielbontenbal/anaconda3/lib/python3.10/site-packages/setuptools/_distutils/core.py\", line 185, in setup\r\n",
      "  \u001b[31m   \u001b[0m     return run_commands(dist)\r\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/michielbontenbal/anaconda3/lib/python3.10/site-packages/setuptools/_distutils/core.py\", line 201, in run_commands\r\n",
      "  \u001b[31m   \u001b[0m     dist.run_commands()\r\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/michielbontenbal/anaconda3/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 969, in run_commands\r\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd)\r\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/michielbontenbal/anaconda3/lib/python3.10/site-packages/setuptools/dist.py\", line 1208, in run_command\r\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\r\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/michielbontenbal/anaconda3/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\r\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\r\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/michielbontenbal/anaconda3/lib/python3.10/site-packages/wheel/bdist_wheel.py\", line 325, in run\r\n",
      "  \u001b[31m   \u001b[0m     self.run_command(\"build\")\r\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/michielbontenbal/anaconda3/lib/python3.10/site-packages/setuptools/_distutils/cmd.py\", line 318, in run_command\r\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\r\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/michielbontenbal/anaconda3/lib/python3.10/site-packages/setuptools/dist.py\", line 1208, in run_command\r\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\r\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/michielbontenbal/anaconda3/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\r\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\r\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/michielbontenbal/anaconda3/lib/python3.10/site-packages/setuptools/_distutils/command/build.py\", line 132, in run\r\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd_name)\r\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/michielbontenbal/anaconda3/lib/python3.10/site-packages/setuptools/_distutils/cmd.py\", line 318, in run_command\r\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\r\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/michielbontenbal/anaconda3/lib/python3.10/site-packages/setuptools/dist.py\", line 1208, in run_command\r\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\r\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/michielbontenbal/anaconda3/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\r\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\r\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/michielbontenbal/anaconda3/lib/python3.10/site-packages/setuptools/command/build_ext.py\", line 84, in run\r\n",
      "  \u001b[31m   \u001b[0m     _build_ext.run(self)\r\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/michielbontenbal/anaconda3/lib/python3.10/site-packages/Cython/Distutils/old_build_ext.py\", line 186, in run\r\n",
      "  \u001b[31m   \u001b[0m     _build_ext.build_ext.run(self)\r\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/michielbontenbal/anaconda3/lib/python3.10/site-packages/setuptools/_distutils/command/build_ext.py\", line 346, in run\r\n",
      "  \u001b[31m   \u001b[0m     self.build_extensions()\r\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/cj/qtbz9fvd3svc0x28yv2756mh0000gn/T/pip-install-depslgqw/xformers_929eab129bff4cf7b54c3df983ea1a88/setup.py\", line 364, in build_extensions\r\n",
      "  \u001b[31m   \u001b[0m     super().build_extensions()\r\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/michielbontenbal/anaconda3/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 843, in build_extensions\r\n",
      "  \u001b[31m   \u001b[0m     build_ext.build_extensions(self)\r\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/michielbontenbal/anaconda3/lib/python3.10/site-packages/Cython/Distutils/old_build_ext.py\", line 195, in build_extensions\r\n",
      "  \u001b[31m   \u001b[0m     _build_ext.build_ext.build_extensions(self)\r\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/michielbontenbal/anaconda3/lib/python3.10/site-packages/setuptools/_distutils/command/build_ext.py\", line 468, in build_extensions\r\n",
      "  \u001b[31m   \u001b[0m     self._build_extensions_serial()\r\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/michielbontenbal/anaconda3/lib/python3.10/site-packages/setuptools/_distutils/command/build_ext.py\", line 494, in _build_extensions_serial\r\n",
      "  \u001b[31m   \u001b[0m     self.build_extension(ext)\r\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/michielbontenbal/anaconda3/lib/python3.10/site-packages/setuptools/command/build_ext.py\", line 246, in build_extension\r\n",
      "  \u001b[31m   \u001b[0m     _build_ext.build_extension(self, ext)\r\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/michielbontenbal/anaconda3/lib/python3.10/site-packages/setuptools/_distutils/command/build_ext.py\", line 549, in build_extension\r\n",
      "  \u001b[31m   \u001b[0m     objects = self.compiler.compile(\r\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/michielbontenbal/anaconda3/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 658, in unix_wrap_ninja_compile\r\n",
      "  \u001b[31m   \u001b[0m     _write_ninja_file_and_compile_objects(\r\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/michielbontenbal/anaconda3/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1554, in _write_ninja_file_and_compile_objects\r\n",
      "  \u001b[31m   \u001b[0m     get_compiler_abi_compatibility_and_version(compiler)\r\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/michielbontenbal/anaconda3/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 337, in get_compiler_abi_compatibility_and_version\r\n",
      "  \u001b[31m   \u001b[0m     if not check_compiler_ok_for_platform(compiler):\r\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/michielbontenbal/anaconda3/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 300, in check_compiler_ok_for_platform\r\n",
      "  \u001b[31m   \u001b[0m     version_string = subprocess.check_output([compiler, '-v'], stderr=subprocess.STDOUT, env=env).decode(*SUBPROCESS_DECODE_ARGS)\r\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/michielbontenbal/anaconda3/lib/python3.10/subprocess.py\", line 421, in check_output\r\n",
      "  \u001b[31m   \u001b[0m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\r\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/michielbontenbal/anaconda3/lib/python3.10/subprocess.py\", line 526, in run\r\n",
      "  \u001b[31m   \u001b[0m     raise CalledProcessError(retcode, process.args,\r\n",
      "  \u001b[31m   \u001b[0m subprocess.CalledProcessError: Command '['c++', '-v']' returned non-zero exit status 1.\r\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\r\n",
      "  \r\n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\r\n",
      "\u001b[31m  ERROR: Failed building wheel for xformers\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for xformers\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to build xformers\n",
      "\u001b[31mERROR: Could not build wheels for xformers, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xformers==0.0.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46e1f36b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxformers\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xformers'"
     ]
    }
   ],
   "source": [
    "import xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6badb1a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, LlamaTokenizer\n\u001b[1;32m      6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m LlamaTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlmsys/vicuna-7b-v1.5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTHUDM/cogvlm-chat-hf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# chat example\u001b[39;00m\n\u001b[1;32m     16\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDescribe this image\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:553\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[1;32m    552\u001b[0m     class_ref \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mauto_map[\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m]\n\u001b[0;32m--> 553\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m \u001b[43mget_class_from_dynamic_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    556\u001b[0m     _ \u001b[38;5;241m=\u001b[39m hub_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(pretrained_model_name_or_path):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/dynamic_module_utils.py:499\u001b[0m, in \u001b[0;36mget_class_from_dynamic_module\u001b[0;34m(class_reference, pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, code_revision, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# And lastly we get the class inside our newly created module\u001b[39;00m\n\u001b[1;32m    487\u001b[0m final_module \u001b[38;5;241m=\u001b[39m get_cached_module_file(\n\u001b[1;32m    488\u001b[0m     repo_id,\n\u001b[1;32m    489\u001b[0m     module_file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    497\u001b[0m     repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[1;32m    498\u001b[0m )\n\u001b[0;32m--> 499\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_class_in_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/dynamic_module_utils.py:199\u001b[0m, in \u001b[0;36mget_class_in_module\u001b[0;34m(class_name, module_path)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03mImport a module on the cache directory for modules and extract a class from it.\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m    `typing.Type`: The class looked for.\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    198\u001b[0m module_path \u001b[38;5;241m=\u001b[39m module_path\u001b[38;5;241m.\u001b[39mreplace(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msep, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 199\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, class_name)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/THUDM/cogvlm-chat-hf/54b93e0af3f1d8badcdeefdb0d26b1dfbc227f7a/modeling_cogvlm.py:19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModelOutputWithPast, CausalLMOutputWithPast\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_cogvlm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CogVLMConfig\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvisual\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EVA2CLIPModel\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelOutput\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/THUDM/cogvlm-chat-hf/54b93e0af3f1d8badcdeefdb0d26b1dfbc227f7a/visual.py:4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01margparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Namespace\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxops\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ACT2FN\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mPatchEmbedding\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xformers'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM, LlamaTokenizer\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained('lmsys/vicuna-7b-v1.5')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'THUDM/cogvlm-chat-hf',\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True\n",
    ").to('cuda').eval()\n",
    "\n",
    "\n",
    "# chat example\n",
    "query = 'Describe this image'\n",
    "image = Image.open(requests.get('https://github.com/THUDM/CogVLM/blob/main/examples/1.png?raw=true', stream=True).raw).convert('RGB')\n",
    "inputs = model.build_conversation_input_ids(tokenizer, query=query, history=[], images=[image])  # chat mode\n",
    "inputs = {\n",
    "    'input_ids': inputs['input_ids'].unsqueeze(0).to('cuda'),\n",
    "    'token_type_ids': inputs['token_type_ids'].unsqueeze(0).to('cuda'),\n",
    "    'attention_mask': inputs['attention_mask'].unsqueeze(0).to('cuda'),\n",
    "    'images': [[inputs['images'][0].to('cuda').to(torch.bfloat16)]],\n",
    "}\n",
    "gen_kwargs = {\"max_length\": 2048, \"do_sample\": False}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, **gen_kwargs)\n",
    "    outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "    print(tokenizer.decode(outputs[0]))\n",
    "\n",
    "# This image captures a moment from a basketball game. Two players are prominently featured: one wearing a yellow jersey with the number\n",
    "# 24 and the word 'Lakers' written on it, and the other wearing a navy blue jersey with the word 'Washington' and the number 34. The player\n",
    "# in yellow is holding a basketball and appears to be dribbling it, while the player in navy blue is reaching out with his arm, possibly\n",
    "# trying to block or defend. The background shows a filled stadium with spectators, indicating that this is a professional game.</s>\n",
    "\n",
    "\n",
    "\n",
    "# vqa example\n",
    "query = 'How many houses are there in this cartoon?'\n",
    "image = Image.open(requests.get('https://github.com/THUDM/CogVLM/blob/main/examples/3.jpg?raw=true', stream=True).raw).convert('RGB')\n",
    "inputs = model.build_conversation_input_ids(tokenizer, query=query, history=[], images=[image], template_version='vqa')   # vqa mode\n",
    "inputs = {\n",
    "    'input_ids': inputs['input_ids'].unsqueeze(0).to('cuda'),\n",
    "    'token_type_ids': inputs['token_type_ids'].unsqueeze(0).to('cuda'),\n",
    "    'attention_mask': inputs['attention_mask'].unsqueeze(0).to('cuda'),\n",
    "    'images': [[inputs['images'][0].to('cuda').to(torch.bfloat16)]],\n",
    "}\n",
    "gen_kwargs = {\"max_length\": 2048, \"do_sample\": False}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, **gen_kwargs)\n",
    "    outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "    print(tokenizer.decode(outputs[0]))\n",
    "\n",
    "# 4</s>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0326b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
